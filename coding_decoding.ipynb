{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "213a3be7-bddb-4a47-b580-86b3bfeae8fb",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<h1>A gentle DIY Introduction to Python and Text Mining For Humanities People</h1>\n",
    "\n",
    "<h2>A Computational Workbook</h2>\n",
    "    <h3> by Anuj Gupta </h3>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7fda77-89c6-48ec-993c-2f384cfbf704",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<h3> INTRODUCTION </h3>\n",
    "<h4>Who am I? </h4>\n",
    "[Content note: put into present tense, adapt for PrW Audience] I am Anuj Gupta, a University Fellow and a Ph.D. student in the Rhetoric, Composition and the Teaching of English program in the Department of English at the University of Arizona. I am interested in UX research, emotions, text analytics, diversity, and accessibility. I bring these interests together by building ed tech tools to help students and teachers to read, write, think, and learn better. I have been trying to teach myself coding over the last two years and consider myself to be an advanced novice. I want to help increase accessibility to coding for everyone, especially Humanists and Social Sciences, so that technology development can be informed by humanistic tenets.You can get in touch with me by email (anujgupta@arizona.edu), twitter (https://twitter.com/mettalrose), and Linkdin (https://www.linkedin.com/in/anuj-gupta-3533541a1/). \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "<h4>What is this workshop about?</h4>\n",
    " Like what the microscope did for biology, and what the telescope did for astronomy, text mining is helping humanists and social scientists explore their data with fresh, new perspectives.  [HGF DISAGREES WITH THIS STATEMENT -> ]<b>Text mining</b> is basically the use of computer languages like Python or R **use language from PrW article here** to find patterns in large textual datasets and it can be a really powerful tool [TOOL IS STILL BANNED] to unpack socio-cultural patterns. Participants in this workshop will learn how to add text mining techniques to their arsenal of research methods.\n",
    "    \n",
    "<h4> What will you achieve by completing this workshop? </h4>\n",
    "\n",
    "By completing this workshop, you will: \n",
    "\n",
    "<b> Goal 1:</b> ENCOUNTER the Natural Language Processing Toolkit (NLTK), a library for performing specific analyses of linguistic (i.e. textual) data \n",
    "\n",
    "\n",
    "<b> Goal 2: </b>PRACTICE how to use computational methods that can help in analysing textual data using several common techniques. [HGF NOTE: model this on PrW text] (frequency distributions, dispersions) and close-reading (collocations, concordances) methods. \n",
    "\n",
    "\n",
    "<b> Goal 3: </b>BRAINSTORM how some of those techniques could be applied to your own work\n",
    "\n",
    "\n",
    "<h4> Before we begin: </h4>\n",
    "    \n",
    "1) Let's take a deep breath. \n",
    "    \n",
    "2) Learning coding using these new methods requires a lot of patience and compassion. My goal is to help reduce coding anxiety that many of us feel. \n",
    "    \n",
    "3) Errors are helpful! \n",
    "    \n",
    "4) You are not expected to learn how to code in this workshop! In fact, we have provided all the available code for you. However, this is a hands-on experience, so be prepared to be an active learner as we go along. Our goal is to provide a space to read and engage with some code in a low-stakes, controlled environment. In our PraxisWiki article we offer some potential next steps for those interested in learning more. \n",
    "\n",
    "5) Its vital to have an exploratory spirit as we go into these unchartered lands. \n",
    "    \n",
    "6) Anuj and Heather share an interest in creating on-ramps for researchers interested in these techniques, but have not encountered the moving parts of working with code before. This notebook, therefore, assumes no prior knowledge or experience working with Python, any other scripting language, or any kind of data at all.  \n",
    "\n",
    "7) [say something about anticipated time commitment - how long should someone devote to this? attempt to do so follows] Covering everything available in this workbook should take about 2 hours total. Each exercise provided builds on the previous activity, but it is fine to do each separate exercise separately. (Do users have to reload if they walk away? etc)\"\n",
    "    \n",
    " </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39467d6d-c58c-4ad1-b8e7-a46f88de4ec3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "***Okay so let's get started!***\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb4e14-5363-4361-a72b-f154706954f1",
   "metadata": {},
   "source": [
    "<h4> First things first. What exactly is text mining and why should we care?</h4>\n",
    "\n",
    "1) [HGF comment: does this get dangerously close to the idea of "test tube chemistry", i.e. the idea that test tubes are the thing that makes chemistry happen?] <b> It's like a microscope + telecscope for the Humanities & Social Science: </b> “Of course computers should be used by scholars in the humanities, just as microscopes should be used by scientists. . . [t]he facts and patterns that they—and often they alone—can reveal should be viewed not as the definitive answers to the questions that humanists have been asking, but rather as the occasion for a whole range of new and more penetrating and more exciting questions” (ACLS, 2006).\n",
    "\n",
    "2) <b> Improves researchers' abilities to find patters in their data </b> You may encounter these methods under the guise of several different names, including but not limited to “text mining” or “text analytics” or “computational text analysis” or “corpus linguistics”. These are (largely) allied terms that refer to the use of computational techniques to find patterns in large textual datasets that are harder to see as linear readers. Moreover, the ability to explore questions that they are interested in at scale means you are no longer stuck counting examples with a highlighter and a pencil. While text mining “cannot reproduce the subtlety of a creative researcher who brings a life of prior associations to their analysis". That said, "computational methods trained on big data can generate many suggestive, subtle associations beyond the sensitivity" of linear readers, who can often miss such details (Evans and Aceves, 2016)\n",
    "\n",
    "3) <b> Its like cognitive prosthetics </b> The difference between human text reading and computerised text mining is: <b> a) Scale: </b> It exponentially increases how many texts can be read and drastically reduces the time required to do so. [HGF comment: is this a good thing? Why/why not? ]<b> b) Patterns: </b> Working with text quantitatively vastly improves the visibility of patterns between the words and sentences that can be found in textual data. (the quote that was here was too long. say it in our own words. Here's a suggestion) Often, these lexical patterns can serve to show us different and valuable evidence about the text that may be less obvious without the aid of such methods to read "through" the text rather than linearly (page 1, word 1, until page 325, word 88,340.)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e0514-7851-43db-817a-85d8967d1b6b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "***Before jumping into feet first, let's understand the platform [to be changed to Colab Notebook] that we are using***\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24565a3-3c75-4828-a9df-cc75224a86b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Part 1: Basic Setup </h1>\n",
    "\n",
    "<p>\n",
    "\n",
    "You have already overcome the first hurdle, by loading this Jupyter Notebook [Colab Notebook -- define what a 'notebook' is]. Think of this as a Google Docs for working with code.> \n",
    "In a notebook, you can write computer code and run it without going into a command line prompt on your own computer. You can also invite other people to collaborate on your work with you. Importantly, any code you run here lives solely in your browser, though once you get comfortable working with scripting languages, you may want to execute these on your own in your own command line prompt.  \n",
    "    <div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484cb76-bf75-41a8-acd4-c447d4b13156",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h5> 1. Let's familiarize ourselves with the affordances of a notebook environment. [Presumably Jupyter and CoLab look like each other?] You should see something similar to the image below at the top right corner of your screen. Slight variations might happen based on the versions we are using.  <-- what does this mean? Unclear ></h5>\n",
    "\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381d34f-6683-48f3-8175-cb24c2fbd9d3",
   "metadata": {},
   "source": [
    "![](Images/jupytertoolbar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71ff6d-792b-481c-a0ab-2f7e6e4bc0fe",
   "metadata": {},
   "source": [
    "<b>The table below shows what each button does. We expect you to primarily engage with the \"play\" or \"run\" button for this tutorial. --- make sure that we are clear about what we WANT them to use... </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442564a-9230-4af8-bb32-959b2022d9ff",
   "metadata": {},
   "source": [
    "\n",
    "Button|Function\n",
    "-|-\n",
    "![](Images/jupytertoolbarsave.png)|This is your save button. You can click this button to save your notebook at any time. Most notebook environments regularly save to the cloud for you (e.g. in the style of Google Docs/Sheets). \n",
    "![](Images/jupytertoolbarnewcell.png)|This is the new cell button. You can click this button any time you want a new cell in your Jupyter Notebook. \n",
    "![](Images/jupytertoolbarcutcell.png)|This is the cut cell button. If you click this button, the cell you currently have selected will be deleted from your Notebook. We don't encourage you use this one in our tutorial. \n",
    "![](Images/jupytertoolbarcopycell.png)|This is the copy cell button. If you click this button, the currently selected cell will be duplicated and stored in your clipboard. \n",
    "![](Images/jupytertoolbarpastecell.png)|This is the paste button. It allows you to paste the duplicated cell from your clipboard into your notebook. \n",
    "![](Images/jupytertoolbarupdown.png)|These buttons allow you to move the location of a selected cell within a Notebook. Simply select the cell you wish to move and click either the up or down button until the cell is in the location you want it to be.\n",
    "![](Images/jupytertoolbarrun.png)|This button will \"run\" your cell, meaning that it will interpret your input and render the output in a way that depends on [what kind of cell] [cell kind] you're using. \n",
    "![](Images/jupytertoolbarstop.png)|This is the stop button. Clicking this button will stop your cell from continuing to run. This brake can be useful if you are trying to execute more complicated code, which can sometimes take a while, and you want to edit the cell before waiting for it to finish rendering. \n",
    "![](Images/jupytertoolbarrestartkernel.png)|This is the restart kernel button. See your kernel documentation for more information.\n",
    "![](Images/jupytertoolbarcellkind.png)|This is a drop down menu which allows you to tell your Notebook how you want it to interpret any given cell. You can read more about the [different kinds of cells] [cell kind] in the following section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb11dba-25fd-4cc0-b8b4-8fb60b0ec14e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h5> 2. We need to learn how to run cells. A cell is simply a space where you can write or edit code. [HGF NOTE: DO WE INTRODUCE CELLS ABOVE?] Try to run the cell below! Just click on the cell and press the ▶ button above to run it.</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0f28a-b14e-4844-b8fb-5155467818f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi! Try to print me.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cfe4e-65b0-48cc-9224-552e10b01114",
   "metadata": {},
   "source": [
    "<b>Excellent! You've just run your first code block. Congratulations. In this command, we simply told the computer to print or show us as the output whatever sentence was written within the bracket. </b>\n",
    "\n",
    "<b> If your code worked properly, you should be seing the sentence \"Hi! Try to print me\" below the code cell. [What happens if it didn't work properly?] </b>\n",
    "\n",
    "Notice, that the input code cell, which was once represented as: \n",
    "\n",
    "`In [ ]:` \n",
    "\n",
    "has now become: \n",
    "\n",
    "`In [1]:` \n",
    "\n",
    "This means your code in that particular cell has run, and it was the first line of code that ran (hence the `1`). This number could vary from notebook to notebook depending on how many cells you have run before this. Since we only have 1 code block so far, the number should be 1. \n",
    "\n",
    "***\n",
    "\n",
    "This code ran quickly. However, sometimes it might take some time to finish. If your code is still running it'll look like this:\n",
    "\n",
    "`In [*]:`\n",
    "\n",
    "That \"star\" in between the brackets means this cell of code is still running. That's OK. We probably won't encounter this outcome in this tutorial, but, in the event it does happen, that is a good moment to take a quick break. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ae7cc-d2d9-4d70-a679-08e3845a06dc",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h5> 3 Understanding errors in python.</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1936e7-d287-45b0-b4ca-9e40f4f7df7c",
   "metadata": {},
   "source": [
    "Python [HGF NOTE: have we introduced the fact this is in python yet] is very good at noticing mistakes. These mistakes can either be a typo that we make or some error in the logic of our commands. Whenever we make such mistakes, Python tells us what we have done by displaying an error message. We need to be able to interpret these messages to fix our errors. \n",
    "\n",
    "In this example, if you spell \"print\" as \"drint\", see what comes back to us where we anticipate a solution from our previous example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be45595-b913-484b-889c-3e14358acbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drint(\"this will not print because of a syntax error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3cb36-bc92-4e76-9bff-1f7664e54fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h5> 4. Finally, let's try and understand in very simple terms what Python is and how it works. HGF NOTE: ah here it is! Does it go here? or should we move elsewhere </h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba58a06-7e22-4b22-96c0-0be44609195c",
   "metadata": {},
   "source": [
    "<p> Python is a programming language. These are not entirely like natural languages, but they allow us to communicate with our computers to increasingly advanced things. It is used widely today for a variety of reasons, not least because :\n",
    "    \n",
    "    \n",
    "    1. Easy to use (define "easy" here??)\n",
    "    2. Lots of help available online. \n",
    "    3. You can easily copy paste other people's code and customize it for your research. <--- is this true of other programming languages? > \n",
    "    4. It is closest to english language and requires less jargon than other languages.<-- hgf note: does this suggest that looking like English is what makes it easy? Some non-native English speakers I know have thoughts about that..... >\n",
    "\n",
    "In this tutorial we obviously don't have time to go into the nuances of how Python works {WHY MENTION IT THEN?} but we will get a very basic surface level understanding of it. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917edde-2741-4fed-967f-7b86f8d01c8e",
   "metadata": {},
   "source": [
    "<p> When we write code in Python, we are simply writing commands in a language that our computer understands. Essentially we are telling the computer to do something for us. For example, let's use Python to tell the computer to do some addition for us:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9d6fc-f401-47fa-945c-866f86ef958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_number = 5\n",
    "second_number = 7 \n",
    "sum = first_number + second_number \n",
    "print(\"the sum of these numbers is \" +str(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e938563-9fa3-4e0c-9533-c1b72338aed8",
   "metadata": {},
   "source": [
    "<p>Excellent! You just ran your first Python code! {HGF NOTE: What makes this different than the one above?} What we did here is simply to create variables called \"first_number\" and \"second_number\" and then to store certain data in them (the numbers 5 and 7). Then we created a new variable called \"sum\" and told the computer that we want to store the sum of those first two variables in that third variable. Finally, we asked the computer to print or display whatever is stored in that third variable called \"sum\" along with some text \"the sum of these numbers is \" in the display space. [HGF NOTE: what about "str"??] If you understand how to use this basic syntax to tell your computer how to store things and how you want to create complex relationships between those stored things, you can do a wide range of powerful things. Everything we do on a computer, including word processing, sending email, looking at social media, video games, sending rockets to the moon, all use this kind of building blocks.  
  },
  {
   "cell_type": "markdown",
   "id": "88e2670f-73b6-44a3-82fc-4d5a1549b5a3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h5> 5. Now that you understand how a basic Python code is written, let's add some complexity by learning what a \"library\" and a \"function\" is.</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7309039-de44-47fa-95c6-fd7891ef4b90",
   "metadata": {},
   "source": [
    "<h5> 4.1 Understanding functions in Python </h5>\n",
    "<p> Rarely do we have to start from scratch when we encounter a programming language. Instead, we can call up bits of code written by other people and supplement it with what our particular needs are. Each bit of code written by someone else that we can use is called a \"function\". For example, in the above cells we just wrote code for a \"sum\" function that could add two numbers together. (Isnt this a key feature of most programming languages, assigning variables and providing an action for them? what makes this task special to python?) Anyone can copy paste it and use it in their work if they want to perform addition between two numbers. <-- this feels too simplistic for a more complex idea?? >\n",
    "    \n",
    "    Function = collection of code that performs a particular role and can be recycled to suit the needs to different coders. \n",
    "    \n",
    "Let's what this looks like in practice.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0cff8f-f6dd-4cde-af02-ae156e282f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i am a human being trying to learn text mining\" [Shouldn't this be 'python'?] \n",
    "count =len(text)\n",
    "print(\"the total number of characters in my text is \" + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa6111-50aa-4346-941d-7bb87da34da9",
   "metadata": {},
   "source": [
    "<p>Here we have used a function called \"len\" which means \"length\" and it basically just calculates the number of characters (alphabets, numbers, punctuation, spaces etc.) that we have stored in a variable. People who created Python wrote the code for this function and simply saved it using the abbreviation \"len\" so we can just type that instead of having to type the corresponding code every time. Programmers like being efficient; shortened forms like this are common as they reduce the amount of typing someone will have to do.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68022347-6c68-43b5-a28a-182869b2d756",
   "metadata": {},
   "source": [
    "<p> Just like this \"len\" function, there are millions of functions for which people across the world have already written for us. We can simply recycle these functions for our purposes. This is kind of like how in academic writing we use and borrow ideas from other scholars, rather than having to invent every single idea from scratch. [DOES THIS MEAN WE HAVE TO CITE OTHERS WORK? WHY/WHY NOT?]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff6791-df67-45fd-8ef8-774706f33cbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "<b>Now you try!</b>\n",
    "\n",
    "In the cells below, first replace the text that says \"input your text here\" with any sentence you like. Then run cell that follows to see how many characters it contains\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61738c78-a418-454b-aaf5-e7816b164f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"input your text here\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf398747-bd14-4ed4-8bbe-51916e02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_count =len(my_text)\n",
    "print(\"the total number of characters in my text is \" + str(my_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a88fba-2674-4fee-b05d-7958d98372bd",
   "metadata": {},
   "source": [
    "<h5> 4.2: Understanding libraries in Python </h5>\n",
    "\n",
    "<p> Now if people simply wrote individual functions and uploaded them (WHERE DO THEY GET UPLOADED?) it could create chaos because there would be billions of functions just floating around in the world, make it very difficult for us to see which functions are useful for us to learn for what purpose. So to create some method in this madness, people have come up with the idea of \"libraries\" which are simply collections of functions clubbed together based on their roles and the domains in which they are useful.  \n",
    "    \n",
    "    Library = collection of functions that perform similar roles (Aren't these also sort of like "packages", bundled for someone to use as one coherent thing? R calls these packages and this is the only initiutive thing that I think R does.)  \n",
    "    \n",
    "Think of functions like a chapter and a library like a book that holds them together (Unpack this more, someone can't reach out and ask for help the way they can in the a workshop you host!)</p>\n",
    "\n",
    "Let's see what this looks like in practice "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bff814-ec36-4d3c-a9ee-384bcbfdf3ab",
   "metadata": {},
   "source": [
    "<p> First let us download a library called \"nltk\" or Natural Language ToolKit (© 2022, NLTK Project) which is a specific library created by these amazing folks (https://www.nltk.org/team.html) to help us do text mining (is this specifically what it does? Be precise). When you run the cell below, you should see a pink box with lots sentences that start with \"Downloading\". Let it run. You'll know its complete when you see \"Done downloading collection book\" appear at the end of the pink box.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf5d39-c757-4b0e-8ee2-4a481bc63184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4539-31ca-49f3-9dad-142a6de9b17c",
   "metadata": {},
   "source": [
    "<h5> 4.3 Exploring the NLTK Library </h5>\n",
    "\n",
    "<p> The NLTK or Natural Language Tool Kit Library © 2022, NLTK Project is a library that the wonderful folks in this team (https://www.nltk.org/team.html)created to help folks like us do some cool text mining!\n",
    "\n",
    "\n",
    "It essentially contains a whole bunch of corpora (collecting of famous texts <--- is this what "A corpus" means?) and text mining functions that we can use to expore them [HGF NOTE: Anuj, at this point we want to be really precise about language -- this is (very likely) the first way people will encounter these terms, and again they can't reach out to ask you about it. Can we motion towards somewhere they can learn more if they're interested here?]</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc1bd3-f659-4d68-bd08-8247394652f9",
   "metadata": {},
   "source": [
    "<h5> 4.4 Loading corpora </h5>\n",
    "<p> A corpus is simply a collection of texts and corpora is plural for corpus. The NLTK library, apart from containing amazing text mining functions {what does this mean? be specific} also contains some really cool in-built corpora that we can use to play around. Let's download a few them by clicking on the cell below. It might take a few seconds to run. You'll know its done running when you see this sentence appear at the bottom:\"text9: The Man Who Was Thursday by G . K . Chesterton 1908\"\n",
    "\n",
    "We encourage you to not worry too much about the specific coding syntax we are using to do this as this workshop is not so much about learning the particular syntax, but instead about gaining familiarity what what it can do. That said, we are confident that you can parse out some of the commands provided -- what might the below code block be asking?</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b413bbd-3b1f-4abe-8714-c19d980032bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdb5b9-4bf2-4e5b-a4cb-25771b7a031f",
   "metadata": {},
   "source": [
    "<p> Say something about getting readers familiar with interpreting the code at some level here. %%  The output suggests that Python has just downloaded 9 corpora for each and stored them in variables ranging from text1 to text9. (What would happen with Text10? is it a requirement that it is a single digit?) Each of these texts offer a different kind of textual data. For example, NLTK includes a famous literary work (like text1 --> Moby Dick) or collection of important socio-political texts (like text4 --> a collection of the inaugural addresses given by all american presidents till today<- pretty sure this one stops with the Clinton years?). Others include a sampling of material scraped from the web (like text5 --> Chat Corpus is a collection of 10,567 posts collected from various online chat services. Text8 --> this is a collection of personal advertisements in newspapers </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e8cfe-a5ab-4d5e-8311-54f9ab3f9423",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5> 4.5 Using NLTK Functions </h5>\n",
    "\n",
    "<p> By now, I hope that you are familiar with what functions are. The NLTK has thousands of functions which help us do different kinds of text mining. Let's see how one of these functions called \"concordance\". Concordancing is a function that allows us to see a window of context in which key words that interest appear in the corpora of our choice</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc297fac-77dd-4434-9498-39979e14ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3686aa-fba1-488b-8481-f8ac5ce2dc07",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Part 2: Text Mining Techniques </h1>\n",
    "\n",
    "<p>\n",
    "\n",
    "Now that we understand the basics, let's start exploring some popular text mining techniques. \n",
    "    \n",
    "Broadly speaking, we can classify most existing text-mining techniques into two categories: \n",
    "    \n",
    "<b> 2.1: Distant Reading (macro-analysis)[telescope]: </b> distant reading \"aims to generate an abstract view by shifting from observing textual content to visualizing global features of a single or of multiple text(s)\" (Janicke et al. 2015, Any Ted Underwood essay up to 2019.)\n",
    "    \n",
    "<b> 2.2: Close reading (micro-analysis) [microscope]:  </b> \"close reading is the thorough interpretation of a text passage by the determination of central themes and the analysis of their development. Moreover, close reading includes the analysis of (1) individuals, events, and ideas, their development and interaction, (2) used words and phrases, (3) text structure and style, and (4) argument patterns\" (Janicke et al. 2015, give an example from SLAT scholarship) \n",
    "    \n",
    "<b> Importance of combining the two approaches </b>: \"distant reading visualizations cannot replace close reading, but they can direct the reader to sections that may deserve further investigation\" (Janicke et al. 2015) \n",
    "    \n",
    "We will now practice several techniques in each of these categories.\n",
    "    \n",
    "Please keep in mind that you are not expected to memorize or fully comprehend the specific syntax of the code that we will now use. Feel welcome, however, to practice reading for context clues about what is happening. We anticipate that you will still get sense of its broad functionality without diving too much into the logic of it. Moreover, we anticipate that you will learn how to customise it (what's it) for your purposes \n",
    "\n",
    "</p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee5cd9-4d13-426a-8e17-704804fcd4f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "       \n",
    "<h3> 2.1 Technique Type: 1: Distant Reading (macro-analysis (Cite Matt Jockers here - this is really his term ))</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f56e1-9adb-4058-ae49-b514449a1282",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.1: Raw Frequencies</h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e0590-4878-4112-8f22-d9565342b165",
   "metadata": {},
   "source": [
    "<h5> 2.1.1.1: What is \"frequency\" and why should we care? </h5>\n",
    "\n",
    "<p> Counting the frequency of words in a text or corpus is the most basic text mining function [HGF NOTE. I thought that was text length?] that can be performed. “High-frequency words are valuable because they have “aboutness”; they suggest what the overall textual object is about (Archer, 2009a, p. 4). The frequency of words is “a relatively objective means of uncovering lexical salience/(frequency) patterns that invite—and frequently repay—further qualitative investigation,” as Dawn Archer (2009a, p. 15) states” (- just cite Dawn here, not where it came from). \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdaffb3-3e52-41bc-972d-71e34662cac7",
   "metadata": {},
   "source": [
    "<b>Coding practice</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553254e-e115-45a9-8bd7-63a8eaa80a83",
   "metadata": {},
   "source": [
    "We can use a simple function called \"count\" to find the frequency or the number of times a word appears in a text. Let's see how it works by using Moby Dick (Text1) as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f7f4e-0113-4bd8-8571-3e9bc8e10484",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.count(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454367d3-68b7-4864-9b2e-60bd467beb2a",
   "metadata": {},
   "source": [
    "This shows us that the word \"God\" appears 132 times in the book Moby Dick. How many times do you think it would appear in the Book of Genesis? More or less than in Moby Dick? Let's find out! Why don't you try to write some code [HGF NOTE: we spend so much time saying they aren't supposed write the code, but now we are asking them?!] now to find the answer to that question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f8ba0-6807-4eb7-a64d-f35e906caeee",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "<b> Now you try! </b>\n",
    "    \n",
    "Write code [SEE ABOVE RE WRITING CODE] in the box below to find out how many times the word \"God\" appears in the Book of Genesis. Remember that in the corpus we downloaded earlier. Don't forget that the Book of Genesis is called text3, as per [TABLE or CODE BLOCK ABOVE - motion so they can scroll up]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d7a7-b718-453e-bfd2-13df73ffd9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f0c2f9-213d-42c0-8fba-3f75c4c6db19",
   "metadata": {},
   "source": [
    "Excellent! Now this simple technique of counting frequency of different words can lead to many interesting insights. \n",
    "\n",
    "For example, things start to get interesting when we compare such frequencies across texts. Let's see how the frequency of the word \"God\" varies across the texts that we have. \n",
    "\n",
    "In the code below, we are creating 9 new variables named a,b,c,d,e,f,g,h,i and in each of them we are storing the total count of the word \"God\" in each of our 9 texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3ad8a-9c42-48e6-bf04-bbee93212597",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = text1.count(\"God\")\n",
    "b = text2.count(\"God\")\n",
    "c = text3.count(\"God\")\n",
    "d = text4.count(\"God\")\n",
    "e = text5.count(\"God\")\n",
    "f = text6.count(\"God\")\n",
    "g = text7.count(\"God\")\n",
    "h = text8.count(\"God\")\n",
    "i = text9.count(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5690c-541e-455c-a287-ebd9dfc5dbc4",
   "metadata": {},
   "source": [
    "Now we are simply printing or displaying these counts using the \"print\" function that we learnt about earlier. \n",
    "\n",
    "Be patient. All of this might seem rudimentary but once it starts to add up, amazing insights come together :)[HGF NOTE: as a read experience, this comes across as being a little pandering! Describe what this is doing instead:] For each text (text1, text2, text3, etc), we have added an argument, by including the ".count" suffix. This tells the computer to first identify the source, and then perform an action (in this case, to count something). We specify "GOD" in the parentheses, telling the computer what we should be counting. These are assigned to individual letters (a, b, c, d, e, f...) with the equals sign, so that the entire function can be called up easily and quickly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eae968-83ed-4aae-adea-1b3cc22f6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Frequency Distribution of the word God in the following texts:\")\n",
    "print(\"Moby Dick = \" + str(a))\n",
    "print(\"Sense and Sensibility = \" + str(b))\n",
    "print(\"The Book of Genesis = \" + str(c))\n",
    "print(\"Inaugural Address Corpus =\" + str(d))\n",
    "print(\"Chat Corpus = \" + str(e))\n",
    "print(\"Monty Python and the Holy Grail = \" + str(f))\n",
    "print(\"Wall Street Journal = \" + str(g))\n",
    "print(\"Personal Corpus = \" + str(h))\n",
    "print(\"The Man Who Was Thursaday = \" + str(i))"
    "Now, describe what we are doing again here. This is hard to understang without some additional scaffolding for beginners. why do we need these extra letters and symbols?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c807cc-bfe6-4527-891e-15454be5061a",
   "metadata": {},
   "source": [
    "<h5> 2.1.1.2 Visualizing raw frequencies using bar graphs </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e48e7d-9bed-47db-82c3-4d38c0753be7",
   "metadata": {},
   "source": [
    "Reading lots data in a list like this can sometimes be a little difficult. Let's try to create a bar graph that will make it easier for us to compare these values to make some comparisons across our texts in a visual way. To do this though, we need to install a few extra libraries apart from NLTK, so let's quickly do that. [WHAT PROGRAMS? WHY? MORE EXTRAPOLATION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07e039-09b1-4ab9-857d-f448e75f19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
   "Why are we including these? what do they do? is this still NLTK? Assume confusion here."
  },
  {
   "cell_type": "markdown",
   "id": "b8d0e061-4b62-481b-b173-4ea6d721419c",
   "metadata": {},
   "source": [
    "Now we need to use code that tells the computer to create a bargraph using the data we have. It's not necessary that you understand the intracacies of this code (you keep saying this but I think this is increasingly not true?). A simple Google search \"how to create bargraphs in python using matplotlib\" gives you the code below and then you can simply customise it to fit your data (What does this mean?). What I mean is that the code that I got from the internet for this had different data and I just replaced that with the names of the texts and frequency of the word \"God\" in them that we have calculated. [This is still too abstract. Give a full description of what this *does*] --> SHOW DON'T TELL THIS NEXT PART!   This is a key skill that we need to master over time -- how to customise existing Python code on the internet for our research purposes. [HGF comment: What do we expect them to do with this? Draw pictures? open a new function? something else? unclear!] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93ad88-ce40-431a-b192-4a9799de3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() #this creates an empty figure with no axis\n",
    "ax = fig.add_axes([1,1,1,1]) #now we are adding x-y axess to the figure in the shape of a square with each side = 1. {WHY?} \n",
    "texts = ['Moby', 'Sense', 'Genesis', 'Inaugural', 'Chat','Monty','Wall','Personal','Man'] #lets create X axis values\n",
    "frequency = [a,b,c,d,e,f,g,h,i] #lets create y axis values\n",
    "ax.bar(texts,frequency) #this command creates a bar graph and specifies what we want on its x and y axis \n",
    "plt.show() #this common prints the visualization we just created. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d34a3d-18d4-4354-9fec-1440d9e0b1da",
   "metadata": {},
   "source": [
    "FIRST, introduce what we just did: We drew a graph to plot the frequency of "God" across our books. Now, we can see  that the word \"God\" appears most frequently in the Book of Genesis, which is well, obvious,[nothing is obvious!!!] because this is a religious book. However, interestingly we see that it appears quite frequently also in a book called Moby Dick and the Inaugural Address speeches of American Presidents. But it is quite infrequent in corpora that represent personal writing and web-based chats. \n",
    "\n",
    "<h5> 2.1.1.3 What kind of inferences could we draw from this? </h5>\n",
    "\n",
    "<b> Hint </b> There can be many inferences that one might draw about what the different frequencies of the word \"God\" reveals to us about these texts and the cultures that they come from. Try to think of categories using which we can diffrentiate between texts in our corpus. What happens when we compare them based on their genre? What happens when we compare them based on their years of publication? What about their cultural contexts? \n",
    "\n",
    " While this might seem quite revealing, to make our inferences more robust, we need to ensure that our data are valid. Right now, we have counted pure frequencies of words but in order to compare them across texts, we need to have normalized frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c70b58-ca31-4398-a7a0-1411473451ef",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.2: Normalized Frequencies</h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d680b1-5473-4ff7-a244-5a1082026b38",
   "metadata": {},
   "source": [
    "<h5> 2.1.2.1: What is normalized freqency and why does it matter? </h5>\n",
    "\n",
    "Image if the word \"God\" appears 10 times in textA and 5 times in textB. On the surface level it would make us infer that the word \"God\" is more important in textA right? However, if textA contains 10,000 words and textB contains only 100 words, we are looking at an uneven comparison. One might assume that the word \"God\" more relevant in textB, but that is not necessarily true. This is why we need to calculate normalized frequencies per million words. [HGF NOTE: I often talk about this using fruit: if we have a orange and we have a watermelon, we know they are different. We will want to project them to the same size, such as a grapefruit -- something that is a standard size between our two other fruit. Now we can make more meaningful comparisons between our objects.] which can be done simply using this formula:\n",
    "\n",
    "<b>Normalized frequency of word w in text T = (number of times word w appears in text T/ total number of words in text T)* 1000000 </b> NOW DESCRIBE WHAT THIS MEANS. We are building a proportional relationship between the times our word of interest appear, by normalizing them to ..... \n",
    "\n",
    "\"Considering frequencies in terms of standardized percentages is often a more sensible way of making sense of data, particularly when comparisons between two or more datasets of different sizes are made\" (Baker, 2006, p.51). SAY THIS IN YOUR OWN WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554476ff-c59d-4999-981b-84f1e34a60b7",
   "metadata": {},
   "source": [
    "To do this, we need to first calculate the total number of words in each of our texts using the \"len\" function we learnt earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167769d-543e-4061-8361-7c8a37ea5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76948d81-61d7-4772-995b-95355d7f09fe",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Now you try. \n",
    "\n",
    "Try to find out total number of units in text2 in the box below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214d278-c069-4afa-9f0f-6e2fc36ed502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee6d3278-44fd-4f28-9173-8cbafac38361",
   "metadata": {},
   "source": [
    "Good job! Now using the code block below let's find out the overall lengths of all the texts in our collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b7f26-3663-4588-bfb6-92dfab7e73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_len=len(text1) \n",
    "b_len=len(text2) \n",
    "c_len=len(text3) \n",
    "d_len=len(text4) \n",
    "e_len=len(text5) \n",
    "f_len=len(text6)\n",
    "g_len=len(text7) \n",
    "h_len=len(text8) \n",
    "i_len=len(text9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d54fea-2d7f-4410-abf7-94c71cd20c2e",
   "metadata": {},
   "source": [
    "Now let's print these results in a way we can read them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad35b6d-f301-4f3c-8cba-88f60ba8ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of words (including punctuations) in Moby Dick is \" +str(a_len))\n",
    "print(\"The length of words (including punctuations) in Sense & Sensibility is \" +str(b_len))\n",
    "print(\"The length of words (including punctuations) in the Book of Genesis is \" +str(c_len))\n",
    "print(\"The length of words (including punctuations) in the Inaugural Address Corpus is \" +str(d_len))\n",
    "print(\"The length of words (including punctuations) in the Chat Corpus is \" +str(e_len))\n",
    "print(\"The length of words (including punctuations) in Monty Python and the Holy Grail is \" +str(f_len))\n",
    "print(\"The length of words (including punctuations) in the Wall Street Journal is \" +str(g_len))\n",
    "print(\"The length of words (including punctuations) in the Personal Corpus is \" +str(h_len))\n",
    "print(\"The length of words (including punctuations) in The Man Who was Thursday is \" +str(i_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938ea1c-ee7d-479a-b9e6-e486d29d975e",
   "metadata": {},
   "source": [
    "Stop -- describe what you just did again. Spell it out -- Where did these sentences come from? Recall the "print" command from earlier -- before you move in to the next part. Now let's calculate the normalized frequency of the word \"God\" in our first book, Moby Dick \n",
    "In the code below, what we are essentially telling Python is to divide the number of times the word \"God\" appears in Moby Dick by the total number of words in Moby Dick, and then multiply that by a million WHY A MILLION? and round it off to the nearest whole number and finally save the result in a variable called n_a (short for normalized frequency of a) and then print it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f336d0-b40a-4d2d-b6b3-64dc701e1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = round(((a/a_len)*1000000))\n",
    "print(n_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dfeab-0892-42ff-9f05-fb361d44d07b",
   "metadata": {},
   "source": [
    "Just like this, we can now save (save in the sense of Word's save feature? OR something else?) normalized frequencies of the word \"God\" in all our texts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b3beb-0e60-47a8-b4b1-069151750753",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = round((a/a_len)*1000000)\n",
    "n_b = round((b/b_len)*1000000)\n",
    "n_c = round((c/c_len)*1000000)\n",
    "n_d = round((d/d_len)*1000000)\n",
    "n_e = round((e/e_len)*1000000)\n",
    "n_f = round((f/f_len)*1000000)\n",
    "n_g = round((g/g_len)*1000000)\n",
    "n_h = round((h/h_len)*1000000)\n",
    "n_i = round((i/i_len)*1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc205c-4b11-4932-b4cc-3e27f232f954",
   "metadata": {},
   "source": [
    "We can also print these like we did above, using the print command (are we still using matplotlib or other libraries now?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51525a-4145-4a98-9fc6-40712c4cf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The normalized frequency of the word God in Moby Dick is \" +str(n_a))\n",
    "print(\"The normalized frequency of the word God in Sense and Sensibility is \" +str(n_b))\n",
    "print(\"The normalized frequency of the word God in the Book of Genesis is \" +str(n_c))\n",
    "print(\"The normalized frequency of the word God in the Inaugural Address Corpus is \" +str(n_d))\n",
    "print(\"The normalized frequency of the word God in the Chat Corpus is \" +str(n_e))\n",
    "print(\"The normalized frequency of the word God in Monty Python and the Holy Grail is \" +str(n_f))\n",
    "print(\"The normalized frequency of the word God in the Wall Street Journal is \" +str(n_g))\n",
    "print(\"The normalized frequency of the word God in Personal Corpus is \" +str(n_h))\n",
    "print(\"The normalized frequency of the word God in The Man Who was Thursday is \" +str(n_a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368afb88-e223-4443-b72b-77d9321f1e6a",
   "metadata": {},
   "source": [
    "<h5> 2.1.2.2: Visualizing normalized frequencies using bar graphs </h5>\n",
    "\n",
    "Let's visualize these results now to make them readable HGF: oh heres where we're pulling those other libraries back in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df0433-f85e-49ac-9823-c6570a63d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure() #create an emppty figure\n",
    "ax = fig.add_axes([1,1,1,1])#add x-y axis to that figure in the shape of a square with each side = 1\n",
    "texts = ['Moby', 'Sense', 'Genesis', 'Inaugural', 'Chat','Monty','Wall','Personal','Man'] #create values for X axis\n",
    "n_frequency = [n_a,n_b,n_c,n_d,n_e,n_f,n_g,n_h,n_i] #create values for Y axis \n",
    "ax.bar(texts,n_frequency) #create a bar graph and specify which is x axis and which is y axis \n",
    "plt.show() #print the bar graph "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a478b-d164-4a0e-a430-b5e587e22b25",
   "metadata": {},
   "source": [
    "<h5> 2.1.2.3: BEFORE WE GET TO THIS! Describe what just happened. What has changed about our results? </h5>\n",
    "\n",
    "<b> How does this change the inferences we had made earlier? Keep this inference in mind </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4ff14-04a3-4d3c-b2ab-8c20e863c5a0",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.3: Dispersion plots WHAT ARE THESE AND WHY? CONNECT TO ABOVE SECTION. </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a07ae-e16a-431f-b92d-e82f90ea27da",
   "metadata": {},
   "source": [
    "<h5> 2.1.3.1 What are dispersion plots? </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9fe2f-f118-49c5-b266-9c5f4ab05f90",
   "metadata": {},
   "source": [
    "\"It is useful to be able to find out how often a particular word occurs in a text, and to display some words that appear immediately adjacent to it. However, sometimes it is worthwhile to see where in a running text a particular word appears in a visual way - from the beginning (left) to the end (right). This positional information can be displayed using a visual called a dispersion plot. Each stripe represents an instance of a word, and each row represents the entire text\" (Bird et al., 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d4eb8-1df3-46c0-b7b5-3b9602a90a11",
   "metadata": {},
   "source": [
    "<h5> 2.1.3.2 How can we create a dispersion plot? </h5>\n",
    "\n",
    "Dispersion plots work especially well if you are trying to find some insights about how frequencies of a word or set of words has changed over time.[Move this to the introduction of this section]. Let's use the striking example that Bird et al. (2019) give in their book. They use text4, which provides copies of inaugural addresses given by American presidents over the years arranged in a chronological order (i.e. the first president's speech is present at the beginning of the book and most recent presidents' speech is present at the end of the book). Let's see what their example shows us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dcfadf-3f42-4f2b-93df-7675c7ec72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dispersion_plot(text, word_list):\n",
    "    # Create a list of indices for each word in the word list\n",
    "    indices_list = []\n",
    "    for word in word_list:\n",
    "        indices_list.append([i for i, x in enumerate(text) if x == word])\n",
    "\n",
    "    # Create a scatter plot for each word\n",
    "    for i, word in enumerate(word_list):\n",
    "        plt.scatter(indices_list[i], [word] * len(indices_list[i]), marker='|', color='blue', s=100)\n",
    "\n",
    "    # Set the x-axis limits to show the entire range of the text\n",
    "    plt.xlim(0, len(text))\n",
    "\n",
    "    # Set the y-axis tick labels to the word list\n",
    "    plt.yticks(word_list, fontsize=12)\n",
    "    plt.gca().yaxis.set_tick_params(pad=50) # Add padding between tick labels\n",
    "\n",
    "    # Set the x-axis label and tick labels\n",
    "    plt.xlabel('Character Index', fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "#This function was created with help from ChatGPT\n",
    "\n",
    "# Example usage\n",
    "text = text4\n",
    "word_list = [\"God\",\"freedom\",\"duties\",\"war\"]\n",
    "create_dispersion_plot(text, word_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8eb2d3ff-a1b4-4950-b983-0171590acd48",
   "metadata": {},
   "source": [
    "OK NOW DESCRIBE WHAT THIS PROCESS DOES. Not intuitive to first time users! What are they key pieces we need them to see and observe how they are being operated on? This example shows how the frequency of the words \"God\", \"freedom\", \"duties\", \"war\" varies in American presidential speeches over time. \n",
    "\n",
    "<h5> 2.1.3.3 What inferences can we draw from this? </h5>\n",
    "    \n",
    "<b>Hint:</b> What does this tell us about how Presidential priorities have been changing over the decades and how is that a reflection of historical shifts?  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e392de3e-2c01-43f7-bd7a-db79d4502f37",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Now you try. \n",
    "\n",
    "Try to find create a dispersion plot for text4 i.e. the presidential corpu by replacing \"enter_word1_here\" and \"enter_word2_here\" in the penultimate line of the code below. Choose at least two words that you think might show any interesting patterns. What insights can we draw from this? OFFER A SUMMARY at the end of some examples someone could pull up. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f9563-16ea-47fc-8ebe-cf0a67fc50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dispersion_plot(text, word_list):\n",
    "    # Create a list of indices for each word in the word list\n",
    "    indices_list = []\n",
    "    for word in word_list:\n",
    "        indices_list.append([i for i, x in enumerate(text) if x == word])\n",
    "\n",
    "    # Create a scatter plot for each word\n",
    "    for i, word in enumerate(word_list):\n",
    "        plt.scatter(indices_list[i], [word] * len(indices_list[i]), marker='|', color='blue', s=100)\n",
    "\n",
    "    # Set the x-axis limits to show the entire range of the text\n",
    "    plt.xlim(0, len(text))\n",
    "\n",
    "    # Set the y-axis tick labels to the word list\n",
    "    plt.yticks(word_list, fontsize=12)\n",
    "    plt.gca().yaxis.set_tick_params(pad=50) # Add padding between tick labels\n",
    "\n",
    "    # Set the x-axis label and tick labels\n",
    "    plt.xlabel('Character Index', fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "#This function was created with help from ChatGPT\n",
    "\n",
    "# Example usage\n",
    "text = text4\n",
    "word_list = [\"enter_word1_here\",\"enter_word2_here\"]\n",
    "create_dispersion_plot(text, word_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1d8b96-897e-45c6-bd13-1c78b1231a8b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.4: Exploring some iconic studies </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e786b-44e1-4daf-8fa8-4b2c5b14bbb1",
   "metadata": {},
   "source": [
    "<b> Good work so far! This very simple technique of counting words that interest us and comparing them across texts can lead to very insightful results. [Again: avoid pandering. Your audience is smart. Give them credit for this -- it might be simple, but anticipate that they might feel overwhelmed at any point] </b>\n",
    "\n",
    "1) Example 1: Let's see a wonderful example of how a researcher did this to map out how corporate organizations did or not did not support the BLM movement on their Twitter profiles, something that could then be used to put pressure on these compabies to support the movement: data and analysis: https://www.kmcelwee.com/fortune-100-blm-report/site/corporate-summaries.html \n",
    "report: https://www.kmcelwee.com/fortune-100-blm-report/site/corporate-summaries.html\n",
    "\n",
    "\n",
    "2) Example 2: Langer et al. (2012) map out how the frequency and diversity of terms that refer to biodiversity has been steadily declining over the last 200 years in Western literature, something that reveals our growing alienation from the natural world https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1002/pan3.10256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1ee84-6c21-4da3-9ca4-bdc1a5962c2c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "       \n",
    "<h3> 2.2 Technique Type: 2: Close reading (micro-analysis) WHAT MAKES THIS DIFFERENT THAN THE ABOVE?</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0e1d9-ef94-4796-8e58-c78f133ee3a2",
   "metadata": {},
   "source": [
    "As humanists and social scientists, thankfully we already know that simply knowing quantative differences between the number of times a word appears in two texts gives us some clues, but does not reveal to use a more in-depth picture. This is why researchers using text mining techniques often mix quantitative approaches with qualitative analyses -- they offer something more familiar to the linear reader. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97184779-f9c1-4c85-b61b-569cf3442110",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<h4> 2.2.1: What are \"condorance lines\" Check spelling, also shouldn't this be earlier?? </h4>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec5994-fe8a-4081-a2dd-dfe41a0499e8",
   "metadata": {},
   "source": [
    "Concordance lines are simply \"a list of all the occurences of a particular search term in a corpus, presented within the context in which they occur; usually a few words to the left and right of the search term\" (Baker, 2006: 71) [HGF comment: Cite the concept not the exact line: say it in your own words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772b6c1-8491-44fa-9704-7ac7f95d0ecd",
   "metadata": {},
   "source": [
    "<h5> 2.2.1.1: Visualizing concordance lines </h5>\n",
    "\n",
    "The NLTK Library has an built-in function we can use to see concordance lines of key terms that interest us\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb445e-30ee-4d80-ad9f-7a074615cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.concordance(\"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c83fe3-d01b-48c9-907a-ce3fef12c32d",
   "metadata": {},
   "source": [
    "As you can see, this function \"concordance\" displays the word we are interested in (here \"woman\" for an example) in a text of our choice (here, text 3 - which is still the Book of Genesis) along with a small slice of the sentences in which that word appears in that text [HGF comment: are they all sentences?]. This allows us to see the semantic context in which our word appears which can be helpful for close-reading [HGF comment: say more about this- unpack it for readers who are unfamiliar with why this might be helpful for close reading. In general, I find that this is the most alien kind of reading for people new to corpus analyses -- it feels somehow wrong to be reading "down" instead of "across"] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99312d-9298-4a86-bf7f-093feb906a1b",
   "metadata": {},
   "source": [
    "<h5> 2.2.1.2: Using concordances to explore our \"God\" inferences </h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88290b-eba0-49c8-9759-543d4cf9de89",
   "metadata": {},
   "source": [
    "<b> Remember the inference you drew earlier based on the frequencies of \"God\" in these texts? Let's now try to gain a more in-depth understanding of it using some close-reading or micro analysis techniques (connect this section above? this feels far away from that example -- group similar things together </b>\n",
    "\n",
    "Let's say that hypothetically, an inference you drew was that the influence of the concept of \"God\" in Western human imagination is reducing over time. While in ancient texts like the Book of Genesis, it is quite high, as we come to more modern texts, its frequency starts to become low. However, in certain genres, like American presidential speeches or existential texts like Moby Dick, it is still relatively high, but in other genres like web-chats or newspaper advertisements it is almost non-existent. \n",
    "\n",
    "<b> Let's now look at some concordance lines to see what contexts the word \"God\" appears in, for these texts. We can test to see if our theory is true: </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dddd8-304f-445a-83d5-e85c05b82b59",
   "metadata": {},
   "source": [
    "Let's first see how "God" appears in Moby Dick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ee051-3293-4a9b-b44d-d222311709eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca8aed-883b-4531-9713-e150ad680cbe",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "<b>Now you try. </b>\n",
    "\n",
    "Now, use the code below to display concordance lines that contain the word \"God\" in a text of your choice from the texts in the NLTK corpus (< reminds them of what this is and where texts came from)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ec651-05df-4f32-adeb-4b6d2939ef55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466e2e8e-6e96-4fbd-a375-6ca6cd489237",
   "metadata": {},
   "source": [
    "<h5> 2.2.1.3: What new inferences can you draw based on such comparison?</h5>\n",
    "\n",
    "<b>Hint:</b> Focus on the difference in the context in which the word \"God\" is used in your chosen text. There is more than one way to approach this question... You might want to focus on the grammatical categories of the word \"God\". Has it been used as a proper noun or a common noun? What kind of tone does the sentence have where it has been used? It is very formal and reverential? Or is it more informal and irreverant? What words (nouns, adjectives, verbs etc.) appear usually near it? What do these differences tell us about the cultures and historical moments in which these texts appear? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4a359-bcf8-4c1e-a978-96c74d1a7f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29657589-ce6d-4430-b356-ccb096603a20",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.2.2: Collocates Analysis</h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75c081-3989-4fa4-8bbf-4bbb72d12027",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.1 What are collocates? </h5>\n",
    "\n",
    "<p> \n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423d676-2b14-409d-b463-64614c17526b",
   "metadata": {},
   "source": [
    "Another very useful technique for close-reading is called collocational analysis. \"When a word regularly appears near another word and the relationship is statistically significant in some way, then such co-occurences are referred to as collocates and the phenomena of certain words frequently occuring next to or near each other is called collocation\" (Baker, 2006: 96) AGAIN: USE YOUR OWN WORDS, CITE THE CONCEPT. This can be especially useful in discourse analysis, when we are trying to find out how certain words or concepts are understood within the world of a text, or within a corpus of texts that represent a particular socio-historical or cultural moment. For example, if we look at a global news corpus, the word \"states\" should appear as an important collocate of the word \"united\", because the word \"united states\" would appear quite frequently in it (what's "it"?) (give second example: In US news documents, you are much more likely to see "united" near "states" than near "manchester"....). HGF nota bene: this is a statistical relationship, unlike clusters; has to do with /likelihood/ of outcomes -- not necessarily grounded in facts. What does a brand new user need to know about this? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5071dbc2-2fa2-422b-9c57-faf01e61307e",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.2 Finding collocates </h5>\n",
    "<h8>All the code snippets below for finding collocates were created with help from Saba (2022)'s code and ChatGPT [HGF note: Do we want to include ChatGPT in this intro? why/why not? Isn't it a potentially confounding variable? Also who is Saba?]</h8>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ea2cf-1918-42a9-a017-83c25bec9d79",
   "metadata": {},
   "source": [
    "Now finding collocates is a slightly more complicated procedure as it involves some slightly complicated statistics and computer programming. [Don't gloss over some of this -- python is good at (certain kinds of) math. ]But thankfully, due to the beauty of Python, we can use code that other people have written. [We have said this already. We can repeat the concept, as long as it is rephrased.] As beginners trying to understand these methods, it is okay to not understand how these procedures are leading to the outputs we are getting, but as you go more in-depth, it is helpful to learn about the intracies of these methods to see how they impact our results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec390c13-e5ae-421f-ae22-a750345f7e1a",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.3 \"God\" in Amerian Presidential Speeches </h5>\n",
    "    \n",
    "The code below finds the most significant 2-word collocates (what does "Significant" mean here?) for a word we choose in text4, which is the Presidential inaugural address corpus <- be careful here to not be slippery about corpus vs text (and vice versa)...  Here I have tweaked HOW DID YOU TWEAK? this code to show us the collocates of the word \"God\". This code also shows us a simple visualization to help us process the results. UNPACK! This is not obvious to observer. Describe each stage of the code, what is doing, and how someone could reasonably 'read along' with what it is saying. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "50c6f270",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.1 Bigram collocates </h5>\n",
    "    \n",
    "The code below finds the most significant 2-word collocates for a word we choose in text4: presidential inaugural address corpus. Here I have tweaked this code to show us the collocates of the word \"God\". This code also shows us a simple visualization to help us process the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98219fff-58b2-45fe-b1c1-4cb85f576078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder # describe me\n",
    "from nltk.probability import FreqDist # describe me\n",
    "import string\n",
    "import matplotlib.pyplot as plt # describe me (didn't we see this library earlier?)\n",
    "\n",
    "# Load text and preprocess it\n",
    "text4 = nltk.corpus.inaugural.raw() #describe me - what's "raw"?? \n",
    "text4_processed = nltk.Text([word.lower() for word in nltk.word_tokenize(text4) if word not in string.punctuation]) #describe me\n",
    "\n",
    "# Find top 10 bigrams that contain 'God'\n",
    "finder = BigramCollocationFinder.from_words(text4_processed)# describe me \n",
    "finder.apply_ngram_filter(lambda w1, w2: 'god' not in (w1, w2))# describe me \n",
    "bigram_list = finder.nbest(nltk.collocations.BigramAssocMeasures().raw_freq, 10) # describe me (why 10?\n",
    "\n",
    "# Get frequency distribution of each bigram in text4 # describe me\n",
    "fdist = FreqDist(bigram for bigram in nltk.bigrams(text4_processed))# describe me \n",
    "bigram_count = [fdist[bigram] for bigram in bigram_list]# describe me \n",
    "\n",
    "# Create a bar chart of the bigram counts\n",
    "plt.bar(range(len(bigram_list)), bigram_count) # describe me\n",
    "plt.xticks(range(len(bigram_list)), bigram_list, rotation=45)# describe me (why 45?)\n",
    "plt.xlabel('Bigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Bigrams Containing \"God\" in Inaugural Addresses') # describe me \n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edc79e6b-e16f-4307-906b-2e4ff945e48a",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.1.1 How to read this output </h5>\n",
    "\n",
    "What we see now is a list of collocates of the word \"God\" in text1. In each bracket, you see one or more words along with the word \"God\" which are its [what is its] collocates. For example, in the bracket ('God','bless'), the verb \"bless\" appears quite frequently to the right of the noun \"God\". "God bless", is a common signoff in this genre of speech,.... describe why this is likely to come together.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "418be152",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.2 Tri-gram collocates </h5>\n",
    "    \n",
    "Let's do the same now for 3-word collocates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01429b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load relevant functions \n",
    "from nltk.collocations import TrigramCollocationFinder # describe me\n",
    "\n",
    "# Load text and preprocess it\n",
    "text4 = nltk.corpus.inaugural.raw()# describe me\n",
    "text4_processed = nltk.Text([word.lower() for word in nltk.word_tokenize(text4) if word not in string.punctuation])# describe me\n",
    "\n",
    "# Find top 10 trigrams that contain 'God'\n",
    "finder = TrigramCollocationFinder.from_words(text4_processed)# describe me\n",
    "finder.apply_ngram_filter(lambda w1, w2, w3: 'god' not in (w1, w2, w3))# describe me\n",
    "trigram_list = finder.nbest(nltk.collocations.TrigramAssocMeasures().raw_freq, 10)# describe me\n",
    "\n",
    "# Get frequency distribution of each trigram in text4\n",
    "fdist = FreqDist(trigram for trigram in nltk.trigrams(text4_processed))# describe me\n",
    "trigram_count = [fdist[trigram] for trigram in trigram_list]# describe me\n",
    "\n",
    "# Create a bar chart of the trigram counts# describe me\n",
    "plt.bar(range(len(trigram_list)), trigram_count)# describe me\n",
    "plt.xticks(range(len(trigram_list)), trigram_list, rotation=45)# describe me\n",
    "plt.xlabel('Trigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Trigrams Containing \"God\" in Inaugural Addresses')# describe me\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "760ca7bd",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.3 Quad-gram collocates </h5>\n",
    "    \n",
    "Let's do the same now for 4-word collocates:You know what my comment is here--DESCRIBE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b9a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load relevant functions \n",
    "from nltk.collocations import QuadgramCollocationFinder\n",
    "\n",
    "# Load text and preprocess it\n",
    "text = nltk.corpus.inaugural.raw()\n",
    "text_processed = nltk.Text([word.lower() for word in nltk.word_tokenize(text) if word not in string.punctuation])\n",
    "\n",
    "# Find top 10 quadgrams that contain 'america'\n",
    "finder = QuadgramCollocationFinder.from_words(text_processed)\n",
    "finder.apply_ngram_filter(lambda w1, w2, w3, w4: 'God' not in (w1, w2, w3, w4))\n",
    "quadgram_list = finder.nbest(nltk.collocations.QuadgramAssocMeasures().raw_freq, 10)\n",
    "\n",
    "# Get frequency distribution of each quadgram in text\n",
    "fdist = FreqDist(quadgram for quadgram in nltk.ngrams(text_processed, 4))\n",
    "quadgram_count = [fdist[quadgram] for quadgram in quadgram_list]\n",
    "\n",
    "# Create a bar chart of the quadgram counts\n",
    "plt.bar(range(len(quadgram_list)), quadgram_count)\n",
    "plt.xticks(range(len(quadgram_list)), quadgram_list, rotation=45)\n",
    "plt.xlabel('Quadgram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Quadgrams Containing \"America\" in Inaugural Addresses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d05b32-d4e8-4921-af89-76d9b6f504fe",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.5: What inferences can we draw from this? </h5>\n",
    "\n",
    "<b> Hint:</b> Think about what the frequent use of the verb \"bless\" next to the noun \"God\" tells us about how American presidents frame the concept of \"God\" discursively or rhetorically in their speeches? [HGF NOTE: Not sure what this means. Rephrase for readers]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17cbfd8-a4a3-4804-80cf-030a286abe4e",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Now you try. \n",
    "\n",
    "In the code below, just change the word \"enter_word_here\" wherever it appears with a word whose collocates you want to explore in the Inaugural Address corpus [text] and then run the cell. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9098293-3784-4745-ac3b-dc1a8ac1d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load text and preprocess it\n",
    "text4 = nltk.corpus.inaugural.raw()\n",
    "text4_processed = nltk.Text([word.lower() for word in nltk.word_tokenize(text4) if word not in string.punctuation])\n",
    "\n",
    "# Find top 10 bigrams that contain 'God'\n",
    "finder = BigramCollocationFinder.from_words(text4_processed)\n",
    "finder.apply_ngram_filter(lambda w1, w2: 'enter_word_here' not in (w1, w2))\n",
    "bigram_list = finder.nbest(nltk.collocations.BigramAssocMeasures().raw_freq, 10)\n",
    "\n",
    "# Get frequency distribution of each bigram in text4\n",
    "fdist = FreqDist(bigram for bigram in nltk.bigrams(text4_processed))\n",
    "bigram_count = [fdist[bigram] for bigram in bigram_list]\n",
    "\n",
    "# Create a bar chart of the bigram counts\n",
    "plt.bar(range(len(bigram_list)), bigram_count)\n",
    "plt.xticks(range(len(bigram_list)), bigram_list, rotation=45)\n",
    "plt.xlabel('Bigram')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top 10 Bigrams Containing \"enter_word_here\" in Inaugural Addresses')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "733b41b9-fd74-489c-b2af-5ca6197446d3",
   "metadata": {},
   "source": [
    "What does this output tell you about how your word of choice has been discursively or rhetorically framed within this corpus of texts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc7396-7d70-4304-92ea-ac83965d1859",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.2.3: Exploring some iconic studies </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828d56c-8a96-4f5d-9e13-4818fe505dc4",
   "metadata": {},
   "source": [
    "<b> Good work so far! Let's explore some iconic studies now that use these techniques </b>\n",
    "\n",
    "1) Example 1: Gabrielatos & Baker (2008) analyse a huge corpus of newspapers in the UK to see what kind of collocates they commonly use to refer to refugees and asylum seekers, and they find consistent patterns like metaphors of \"water\" used to construct refugees (e.g. \"refugees are flooding the country\") to create a cognitive frame whereby the public thinks of them as a natural disaster which is plaguing their country: https://journals.sagepub.com/doi/abs/10.1177/0075424207311247\n",
    "\n",
    "2) Example 2: Thomas and Droge (2022) analyse a huge corpus of newspapers in the US to see what kind of topics are commonly used whenever the word \"Humanities\" is referred to. They use a Machine Learning technique called topic modelling which is like a fancier version of collocates analysis. They find that instead of Humanities bein referred to only in terms of \"crisis\", which is what academics believe is the public discourse around this discipline, there are a range of interesting topics that are prevalent. Let's explore their findings here: https://we1s.ucsb.edu/research/we1s-findings/key-findings/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44e2b2-8301-4510-a44c-b6e724c1c2ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Part 3: Applying to your work </h1>\n",
    "\n",
    "\n",
    "<b> How do you think some of these techniques might apply to your work? </b>\n",
    "    \n",
    "<b>Some suggestions:</b>\n",
    "    \n",
    "<u>1. Play around with existing corpora and textual datasets: </u>\n",
    "\n",
    "https://www.kaggle.com/datasets\n",
    "    \n",
    "https://crow.corporaproject.org/\n",
    "\n",
    "https://elicorpora.info/main\n",
    "    \n",
    "https://dataverse.harvard.edu/dataverse/gwu-libraries\n",
    "    \n",
    "https://catalog.docnow.io/\n",
    "    \n",
    "<u> 2. Build your own corpus of texts: </u>\n",
    "    \n",
    "https://writecrow.org/ciabatta/\n",
    "    \n",
    "<u> 3. Read journals that publish research with text mining techniques for Humanities/Social Sciences </u>\n",
    "    \n",
    "Journal of Cultural Analytics: https://culturalanalytics.org/issues  \n",
    "    \n",
    "Digital Humanities Quarterly: http://digitalhumanities.org/dhq/\n",
    "    \n",
    "Journal of Writing Analytics:  \n",
    "\n",
    "<u> 4. Web resoures with beginner friendly lessons on text-mining </u>\n",
    "    \n",
    "Where to start with text mining by Ted Underwood:https://tedunderwood.com/2012/08/14/where-to-start-with-text-mining/\n",
    "    \n",
    "University of Arizona Library's Guide to Text-Mining: https://libguides.library.arizona.edu/text-mining\n",
    "    \n",
    "JSTOR's Text Analysis Pedagogy Institute's Resources: https://labs.jstor.org/projects/text-analysis-pedagogy-institute-2/\n",
    "    \n",
    "Introduction to Cultural Analytics & Python: https://melaniewalsh.github.io/Intro-Cultural-Analytics/welcome.html \n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0bd2c1e-180c-4ae5-b7a8-801c90b70445",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Part 4: Feedback </h1>\n",
    "\n",
    "<p>\n",
    "\n",
    "To help me improve the design of this workshop, please fill out this short feedback survey. It will take 3-4 minutes: https://forms.gle/NzVpUdCx8KqHauL36\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ee42d-6c0b-4a83-ad50-dc58838be157",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2> Acknowledgements</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09fc73-d324-4961-a404-6db406b8f8ea",
   "metadata": {},
   "source": [
    "1. This resource was developed as part of the Workshop Series from the Digital Scholarship and Data Science Fellowship Program funded by the University Libraries at the University of Arizona. You can learn more about this program by clicking here: https://data.library.arizona.edu/data-science/ds2f\n",
    " \n",
    "2. I'm grateful to Jeffrey Oliver and Megan Senseney for their inspiring mentorship and well-scaffolded year long training program that helped me produce this resource. I'm also thankful to Jim Martin, Yvonne Mery, Leslie Sult, and Cheryl Casey for their insightful lectures on various aspects of data science, digital pedagogy, and open educatioanl resource production. \n",
    " \n",
    "3. I'm especially thankfully to Prof. Charlie Gomez and his amazing class INFO 514: Computational Social Science at the University of Arizona. A lot of the content for this notebook, especially the introduction to Jupypter Notebooks as well as the markdown code for this notebook have been adapted, with permission, from his class notebooks. If you are a University of Arizona student interested in learning these skills, I strongly recommend taking this class or auditing it.\n",
    "\n",
    "4. I'm extremely grateful to Prof. Shelley Staples and Mark Fuller at the CROW Lab with whom I did an indepdent study during Spring 2022 and learnt the ropes of text mining and corpus linguistics. \n",
    "\n",
    "5. I would also like to thank the RCTE Program and its faculty who created the Immersive Cultural Requirement (ICR) for PhD students which requires them to immerse themselves in a culture, learn new skills and reciprocate. That programmatic requirement encouraged me to immerse myself in the text-mining community, learn coding, and am now trying to reciprocate through such resources and workshops. \n",
    "\n",
    "6. I'm also grateful to JSTOR's TAPI summer institute, University of Birmingham's Corpus Linguistics summer institute, and the ISTA 130: Introduction to Computational Thinking class at the University of Arizona, all avenues where I learnt a lot of text-mining skills. \n",
    "\n",
    "7. I'm also grateful to ChatGPT for helping me debug some of my code.\n",
    "\n",
    "8. Last but not the least, I want to also acknowledge the amazing coding community online on Github, Stackoverflow, and Youtube who are so generous with sharing resources and helping out beginners through threads and discussions. The academic world could really benefit a lot from following the open-sourced and compassionate nature of knowledge building that these communities follow online."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326dcde4-354e-4489-b396-ff495b740aa4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2> Works Cited</h2>\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f25608-5c9d-4cfc-9508-e01584fa460a",
   "metadata": {},
   "source": [
    "1. ACLS. (2006). Our Cultural Commonwealth: The report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. ACLS. https://www.ideals.illinois.edu/items/199 <br>\n",
    "    \n",
    "2. Evans, J. A., & Aceves, P. (2016). Machine Translation: Mining Text for Social Theory. Annual Review of Sociology, 42(1), 21–50. https://doi.org/10.1146/annurev-soc-081715-074206<br>\n",
    "    \n",
    "3. Bird, Steven, Ewan Klein, and Edward Loper (2009), Natural Language Processing with Python, O'Reilly Media.<br>\n",
    "    \n",
    "4. Baker, P. (2006). Using corpora in discourse analysis. Continuum.\n",
    "\n",
    "5. Gabrielatos, C., & Baker, P. (2008). Fleeing, Sneaking, Flooding: A Corpus Analysis of Discursive Constructions of Refugees and Asylum Seekers in the UK Press, 1996-2005. Journal of English Linguistics, 36(1), 5–38. https://doi.org/10.1177/0075424207311247\n",
    "\n",
    "\n",
    "6. Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming with NumPy. Nature 585, 357–362 (2020). DOI: 10.1038/s41586-020-2649-2. (Publisher link)\n",
    "\n",
    "7. Jänicke, S., Franzini, G., Cheema, M. F., & Scheuermann, G. (2015). On Close and Distant Reading in Digital Humanities: A Survey and Future Challenges. Eurographics Conference on Visualization, 21.\n",
    "\n",
    "8. J. D. Hunter, \"Matplotlib: A 2D Graphics Environment\", Computing in Science & Engineering, vol. 9, no. 3, pp. 90-95, 2007.\n",
    "\n",
    "9. Langer, L., Burghardt, M., Borgards, R., Böhning‐Gaese, K., Seppelt, R., & Wirth, C. (2021). The rise and fall of biodiversity in literature: A comprehensive quantification of historical changes in the use of vernacular labels for biological taxa in Western creative literature. People and Nature, 3(5), 1093–1109. https://doi.org/10.1002/pan3.10256\n",
    "\n",
    "10. McElwee. (2021). The Fortune 100 & Black Lives Matter. Kmcelwee. https://www.kmcelwee.com/fortune-100-blm-report/site/corporate-summaries.html\n",
    "\n",
    "\n",
    "11. Sabba. (2022). NLTK collocations for specific words. https://9to5answer.com/nltk-collocations-for-specific-words\n",
    "\n",
    "12. Schreiber, J., & Melon�on, L. (Eds.). (2022). Assembling Critical Components: A Framework for Sustaining Technical and Professional Communication. The WAC Clearinghouse; University Press of Colorado. https://doi.org/10.37514/TPC-B.2022.1381\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83fe70-b648-419c-833e-4c1ba6e22182",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2> Copyright information </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec25394-8dbc-45e2-9cea-6c5f0885fb98",
   "metadata": {},
   "source": [
    "Shield: [![CC BY 4.0][cc-by-shield]][cc-by]\n",
    "\n",
    "This work is licensed to Anuj Gupta under a\n",
    "[Creative Commons Attribution 4.0 International License][cc-by].\n",
    "\n",
    "[![CC BY 4.0][cc-by-image]][cc-by]\n",
    "\n",
    "[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png\n",
    "[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
